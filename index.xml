<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ICERM Generative Models Discussion Group</title>
    <link>https://stanniszhou.github.io/discussion-group/</link>
    <description>Recent content on ICERM Generative Models Discussion Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Aug 2017 21:38:52 +0800</lastBuildDate>
    
	<atom:link href="https://stanniszhou.github.io/discussion-group/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Papers</title>
      <link>https://stanniszhou.github.io/discussion-group/papers/</link>
      <pubDate>Sun, 20 Aug 2017 21:38:52 +0800</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/papers/</guid>
      <description>Papers Here is a list of suggested papers, loosely organized by topic.
Compositional Generative Models  Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. “Human-Level Concept Learning through Probabilistic Program Induction.” Science 350 (6266): 1332–38. link
 George, D., W. Lehrach, K. Kansky, M. Lázaro-Gredilla, C. Laan, B. Marthi, X. Lou, et al. 2017. “A Generative Vision Model That Trains with High Data Efficiency and Breaks Text-Based CAPTCHAs.</description>
    </item>
    
    <item>
      <title>Schedule</title>
      <link>https://stanniszhou.github.io/discussion-group/schedule/</link>
      <pubDate>Sun, 20 Aug 2017 21:38:52 +0800</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/schedule/</guid>
      <description>Upcoming Schedule Computability of Conditional Probability When: Thursday 2019-04-25 10 am to 11 am
Where: ICERM 11th Floor Lecture Hall
Presenter: Guangyao Zhou
Scribe: Guangyao Zhou
References:
 Ackerman, Nathanael L., Cameron E. Freer, and Daniel M. Roy. 2010. “On the Computability of Conditional Probability.” arXiv [math.LO]. arXiv. link
 Ackerman, N. L., C. E. Freer, and D. M. Roy. 2011. “Noncomputable Conditional Distributions.” In 2011 IEEE 26th Annual Symposium on Logic in Computer Science, 107–16.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://stanniszhou.github.io/discussion-group/about/</link>
      <pubDate>Sun, 20 Aug 2017 21:38:52 +0800</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/about/</guid>
      <description>The ICERM Generative Models Discussion Group is part of the Semester Program in Computer Vision at ICERM. We meet in most weeks in the Spring 2019 semester in the 11th floor lecture hall at ICERM. The discussion group will focus on the general research area of probabilistic generative models.
For questions, please contact Guangyao Zhou.</description>
    </item>
    
    <item>
      <title>Digging Deeper Into Flow-based Generative Models</title>
      <link>https://stanniszhou.github.io/discussion-group/post/ffjord/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/ffjord/</guid>
      <description>Summary
In this week&amp;rsquo;s meeting, we discussed free-form Jacobian of reversible dynamics (FFJORD)[1], and the closely related neural ordinary differential equation (neural ODE)[2]. In this blog post, we summarize the main points of these two papers.
 Overview Recall that the essential idea of flow-based generative models is to model a complicated target distribution as the result of applying a reversible differentiable transformation to some simple base distribution. The base distribution should be easy to sample from, so that we can apply the differentiable transformation and get a sample from the target distribution.</description>
    </item>
    
    <item>
      <title>GANs that work well empirically</title>
      <link>https://stanniszhou.github.io/discussion-group/post/gan-empirical/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/gan-empirical/</guid>
      <description>Overview Generative Adversarial Networks (GANs) are a class of deep learning methods which is first proposed by Ian Goodfellow and other researchers at the University of Montreal in 2014 [1]. Two neural networks, a generator, and a discriminator learn in a zero-sum game framework.
The loss formulation of GAN is as follows:
$$ \min_{G} \ \max_{D}V(D,G)= \mathbb{E}_{x\sim p_{data}(x)}\big[ \log D(x) \big] + \mathbb{E}_{z\sim p_{z}(z)} \big[ \log (1- D(G(z))) \big]$$</description>
    </item>
    
    <item>
      <title>Overview of Deep Generative Models</title>
      <link>https://stanniszhou.github.io/discussion-group/post/dgm-overview/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/dgm-overview/</guid>
      <description>Summary
This week Stannis gave a high-level overview of three popular families of deep generative models. The discussion is mainly based on the original papers [1][2]. The goal is to point out the commonalities and differences between these models, and have a detailed discussion on the different learning methods employed by these models.
 Overview When using latent variable models for probabilistic modeling, the objects of interest are the latent variables (which we denote by $z$), and the observed variables (which we denote by $x$).</description>
    </item>
    
    <item>
      <title>Tutorial on Probabilistic Programming</title>
      <link>https://stanniszhou.github.io/discussion-group/post/ppls/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/ppls/</guid>
      <description>Summary
This week, Daniel gave a tutorial on probabilistic programming and its use in generative modeling.
 What is a PPL? Probabilistic programming languages (PPLs) leverage powerful programming concepts such as recursion, abstraction and modularity to define and sample from user-specified distributions and perform inference on statistical models.
For example, here is a program written in WebPPL:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  var geometric = function() { return flip(.</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ICERM Generative Models Discussion Group</title>
    <link>https://stanniszhou.github.io/discussion-group/post/</link>
    <description>Recent content in Posts on ICERM Generative Models Discussion Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 04 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://stanniszhou.github.io/discussion-group/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Digging Deeper Into Flow-based Generative Models</title>
      <link>https://stanniszhou.github.io/discussion-group/post/ffjord/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/ffjord/</guid>
      <description>Summary
In this week&amp;rsquo;s meeting, we discussed free-form Jacobian of reversible dynamics (FFJORD)[1], and the closely related neural ordinary differential equation (neural ODE)[2]. In this blog post, we summarize the main points of these two papers.
 Overview Recall that the essential idea of flow-based generative models is to model a complicated target distribution as the result of applying a reversible differentiable transformation to some simple base distribution. The base distribution should be easy to sample from, so that we can apply the differentiable transformation and get a sample from the target distribution.</description>
    </item>
    
    <item>
      <title>GANs that work well empirically</title>
      <link>https://stanniszhou.github.io/discussion-group/post/gan-empirical/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/gan-empirical/</guid>
      <description>Overview Generative Adversarial Networks (GANs) are a class of deep learning methods which is first proposed by Ian Goodfellow and other researchers at the University of Montreal in 2014 [1]. Two neural networks, a generator, and a discriminator learn in a zero-sum game framework.
The loss formulation of GAN is as follows:
$$ \min_{G} \ \max_{D}V(D,G)= \mathbb{E}_{x\sim p_{data}(x)}\big[ \log D(x) \big] + \mathbb{E}_{z\sim p_{z}(z)} \big[ \log (1- D(G(z))) \big]$$</description>
    </item>
    
    <item>
      <title>Overview of Deep Generative Models</title>
      <link>https://stanniszhou.github.io/discussion-group/post/dgm-overview/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/dgm-overview/</guid>
      <description>Summary
This week Stannis gave a high-level overview of three popular families of deep generative models. The discussion is mainly based on the original papers [1][2]. The goal is to point out the commonalities and differences between these models, and have a detailed discussion on the different learning methods employed by these models.
 Overview When using latent variable models for probabilistic modeling, the objects of interest are the latent variables (which we denote by $z$), and the observed variables (which we denote by $x$).</description>
    </item>
    
    <item>
      <title>Tutorial on Probabilistic Programming</title>
      <link>https://stanniszhou.github.io/discussion-group/post/ppls/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://stanniszhou.github.io/discussion-group/post/ppls/</guid>
      <description>Summary
This week, Daniel gave a tutorial on probabilistic programming and its use in generative modeling.
 What is a PPL? Probabilistic programming languages (PPLs) leverage powerful programming concepts such as recursion, abstraction and modularity to define and sample from user-specified distributions and perform inference on statistical models.
For example, here is a program written in WebPPL:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  var geometric = function() { return flip(.</description>
    </item>
    
  </channel>
</rss>